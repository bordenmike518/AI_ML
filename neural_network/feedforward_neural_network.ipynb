{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "    def __init__(self, struct, prevCount):\n",
    "        self.prevCount = prevCount\n",
    "        self.count = struct['neurons']\n",
    "        self.neurons = None\n",
    "        self.errors = None\n",
    "        self.alpha = 0.25\n",
    "        self.activationDerivative = self.identity_derivative\n",
    "        if(struct['type'] != 'input'):\n",
    "            self.synapses = np.random.normal(0.0, 0.01, (self.count, self.prevCount))\n",
    "            self.bias = np.ones((self.count, 1))\n",
    "            self.learningRate = struct['learningRate']\n",
    "            self.dropoutRate = struct['dropoutRate']\n",
    "            self.dropout = np.array([[self.learningRate]*prevCount]*self.count)\n",
    "            if (struct['type'] != 'custom'):\n",
    "                if (struct['activation'] == 'identity'):\n",
    "                    self.activation = self.identity\n",
    "                    self.activationDerivative = self.identity_derivative\n",
    "                elif (struct['activation'] == 'binarystep'):\n",
    "                    self.activation = self.binarystep\n",
    "                    self.activationDerivative = self.binarystep_derivative\n",
    "                elif (struct['activation'] == 'sigmoid'):\n",
    "                    self.activation = self.sigmoid\n",
    "                    self.activationDerivative = self.sigmoid_derivative\n",
    "                elif (struct['activation'] == 'tanh'):\n",
    "                    self.activation = self.tanh\n",
    "                    self.activationDerivative = self.tanh_derivative\n",
    "                elif (struct['activation'] == 'arctan'):\n",
    "                    self.activation = self.arctan\n",
    "                    self.activationDerivative = self.arctan_derivative\n",
    "                elif (struct['activation'] == 'ReLU'):\n",
    "                    self.activation = self.ReLU\n",
    "                    self.activationDerivative = self.ReLU_derivative\n",
    "                elif (struct['activation'] == 'leakyReLU'):\n",
    "                    self.activation = self.leakyReLU\n",
    "                    self.activationDerivative = self.leakyReLU_derivative\n",
    "                elif (struct['activation'] == 'ELU'):\n",
    "                    self.activation = self.ELU\n",
    "                    self.activationDerivative = self.ELU_derivative\n",
    "                elif (struct['activation'] == 'softplus'):\n",
    "                    self.activation = self.softplus\n",
    "                    self.activationDerivative = self.softplus_derivative\n",
    "                elif (struct['activation'] == 'softmax'):\n",
    "                    self.activation = self.softmax\n",
    "                    self.activationDerivative = self.softmax_derivative\n",
    "                else:\n",
    "                    print('ERROR: NeuronLayer requires valid activation.')\n",
    "                if (struct['type'] == 'output'):\n",
    "                    if (struct['loss'] == 'mse'):\n",
    "                        self.loss = self.MSE\n",
    "                    elif (struct['loss'] == 'msePrime'):\n",
    "                        self.loss = self.MSEPrime\n",
    "                    elif (struct['loss'] == 'mae'):\n",
    "                        self.loss = self.MAE\n",
    "                    elif (struct['loss'] == 'hinge'):\n",
    "                        self.loss = self.hinge\n",
    "                    elif (struct['loss'] == 'sigmoidCrossEntropy'):\n",
    "                        self.loss = self.sigmoidCrossEntropy\n",
    "                    else:\n",
    "                        print('ERROR: NeuronLayer requires valid loss function.')\n",
    "            else:\n",
    "                if ('activation' in struct):\n",
    "                    self.activation = struct['activation']\n",
    "                    self.activationDerivative = struct['activationDerivative']\n",
    "                if ('loss' in struct):\n",
    "                    self.loss = struct['loss']\n",
    "\n",
    "    def identity(self, A):\n",
    "        return A\n",
    "    \n",
    "    def identity_derivative(self, A):\n",
    "        return np.ones(A.shape)\n",
    "    \n",
    "    def binarystep(self, A):\n",
    "        return np.where(A < 0, 0, 1)\n",
    "    \n",
    "    def binarystep_derivative(self, A):\n",
    "        return np.ones(A.shape)\n",
    "\n",
    "    def sigmoid(self, A):\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "\n",
    "    def sigmoid_derivative(self, A):\n",
    "        return A * (1 - A)\n",
    "\n",
    "    def tanh(self, A):\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def tanh_derivative(self, A):\n",
    "        return 1 - np.power(np.tanh(A), 2)\n",
    "\n",
    "    def arctan(self, A):\n",
    "        return np.arctan(A)\n",
    "    \n",
    "    def arctan_derivative(self, A):\n",
    "        return 1 / (np.power(A, 2) + 1)\n",
    "\n",
    "    def ReLU(self, A):\n",
    "        return np.where(A >= 0, A, 0)\n",
    "\n",
    "    def ReLU_derivative(self, A):\n",
    "        return np.where(A >= 0, 1, 0)\n",
    "\n",
    "    def leakyReLU(self, A):\n",
    "        return np.where(A >= 0, A, np.multiply(self.alpha,A))\n",
    "\n",
    "    def leakyReLU_derivative(self, A):\n",
    "        return np.where(A >= 0, 1, self.alpha)\n",
    "\n",
    "    def ELU(self, A):\n",
    "        return np.where(A >= 0, A, np.multiply(self.alpha, np.exp(A)-1))\n",
    "\n",
    "    def ELU_derivative(self, A):\n",
    "        return np.where(A >= 0, 1, np.multiply(\n",
    "            self.alpha, np.exp(A)-1) + self.alpha)\n",
    "\n",
    "    def softplus(self, A):\n",
    "        return np.log(1 + np.exp(A))\n",
    "\n",
    "    def softplus_derivative(self, A):\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "    \n",
    "    def softmax(self, A):\n",
    "        e = np.exp(A - np.max(A))\n",
    "        return e / e.sum()\n",
    "    \n",
    "    def softmax_derivative(self, A):\n",
    "        return A\n",
    "\n",
    "    def MSE(self, y, yhat):\n",
    "        return np.power((y-yhat - y),2) / y.size\n",
    "\n",
    "    def MSEPrime(self, y, yhat):\n",
    "        return y - yhat\n",
    "\n",
    "    def MAE(self, y, yhat):\n",
    "        return np.absolute(y - yhat)\n",
    "\n",
    "    def hinge(self, y, yhat):\n",
    "        return np.where(1-(y*yhat) > 0, 1-(y*yhat), 0)\n",
    "    \n",
    "    def entropy(self, yhat):\n",
    "        return np.where(p != 0, p*np.log2(p), 0)\n",
    "\n",
    "    def crossEntropy(self, y, yhat):\n",
    "        return np.max(yhat, 0) - yhat * y + np.log2(1 + np.exp(-yhat))\n",
    "\n",
    "    def KLDivergence(self, y, yhat):\n",
    "        return self.crossEntropy(y, yhat) - self.entropy(yhat)\n",
    "    \n",
    "    def logLikelihood(self, y, yhat):\n",
    "        return np.where((y/2)-0.5+yhat > 0, -np.log(np.abs((y/2)-0.5+yhat)), 0)\n",
    "\n",
    "    def dropoutUpdate(self, level=0):\n",
    "        if (level == 0):\n",
    "            if (self.dropoutRate != 0):\n",
    "                self.dropout = self.learningRate * np.random.choice([0, 1],\n",
    "                    size=(self.count, self.prevCount),\n",
    "                    p=[self.dropoutRate, 1-self.dropoutRate]).astype(np.bool)\n",
    "        elif(level == 1):\n",
    "            np.random.shuffle(self.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, networkStruct):\n",
    "        self.layers = len(networkStruct)\n",
    "        self.network = list()\n",
    "        self.accHistory = list()\n",
    "        self.confusionMatrix = None\n",
    "        self.network = np.array(\n",
    "            [NeuronLayer(networkStruct[0], 0)])\n",
    "        for i in range(1, self.layers):\n",
    "            np.append(self.network,\n",
    "                [NeuronLayer(networkStruct[i],\n",
    "                    networkStruct[i-1]['neurons'])])\n",
    "\n",
    "    def feedForward(self, data, target=None):\n",
    "        self.network[0].neurons = np.array([])\n",
    "        for l in range(1,self.layers):\n",
    "            self.network[l].neurons = np.array([])\n",
    "            self.network[l].errors = np.array([])\n",
    "        self.network[0].neurons = np.array(data)\n",
    "        for i in range(1, self.layers):\n",
    "            print(self.network[i].synapses.shape)\n",
    "            print(self.network[i-1].neurons.shape)\n",
    "            print(np.dot(self.network[i].synapses,\n",
    "                    self.network[i-1].neurons).shape)\n",
    "            print(self.network[i].bias.shape)\n",
    "            np.append(self.network[i].neurons,\n",
    "                [self.network[i].activation(\n",
    "                    np.dot(self.network[i].synapses,\n",
    "                        self.network[i-1].neurons) + \\\n",
    "                            self.network[i].bias)])\n",
    "        if (type(target) != type(None)):\n",
    "            np.append(self.network[-1].errors,\n",
    "                [self.network[-1].loss(\n",
    "                    target, \n",
    "                    self.network[-1].neurons) * \\\n",
    "                self.network[-1].activationDerivative(\n",
    "                    self.network[-1].neurons)])\n",
    "#         elif (len(data.shape) == 3):\n",
    "#             for i in range(1, self.layers):\n",
    "#                 self.network[i].neurons = self.network[i].activation(\n",
    "#                      np.transpose(np.dot(self.network[i].synapses,\n",
    "#                         self.network[i-1].neurons), (1,0,2)) + \\\n",
    "#                             self.network[i].bias)\n",
    "#             if (type(target) != type(None)):\n",
    "#                 self.network[-1].errors = self.network[-1].loss(\n",
    "#                     target,\n",
    "#                     self.network[-1].neurons) * \\\n",
    "#                 self.network[-1].activationDerivative(\n",
    "#                     self.network[-1].neurons)\n",
    "\n",
    "    def backpropagation(self):\n",
    "        for i in reversed(range(1, self.layers)):\n",
    "            self.network[i].synapses += self.network[i].dropout * \\\n",
    "                np.average(np.dot(self.network[i].errors,\n",
    "                    self.network[i-1].neurons.T), axis=0)\n",
    "            self.network[i].bias += self.network[i].dropout * \\\n",
    "                np.average(np.dot(self.network[i].errors,\n",
    "                    self.network[i].errors.T), axis=0)\n",
    "            self.network[i-1].errors = np.multiply(\n",
    "                np.dot(self.network[i].synapses.T,\n",
    "                    self.network[i].errors),\n",
    "                self.network[i-1].activationDerivative(\n",
    "                    self.network[i-1].neurons))\n",
    "\n",
    "    def train(self, trainLabels, trainData, epochs=1, \n",
    "              testLabels=[], testData=[], minibatch=50):\n",
    "        zippedData = list(zip(trainLabels,trainData))\n",
    "        for e in range(epochs):\n",
    "            print('\\t-- Epoch {}'.format(e+1))\n",
    "            for i in range(1, self.layers):\n",
    "                self.network[i].dropoutUpdate(0)\n",
    "            np.random.shuffle(zippedData)\n",
    "            for j in range(0, len(zippedData), minibatch):\n",
    "                label, data = zip(*zippedData[j: j+minibatch])\n",
    "                label, data = np.array(label), np.array(data)\n",
    "                target = self.oneHotEncode(label)\n",
    "#                 print(data.reshape(minibatch,data.shape[1],1).shape)\n",
    "#                 print(target.shape)\n",
    "                data = data.reshape(minibatch,data.shape[1],1)\n",
    "                self.feedForward(data, target)\n",
    "                self.backpropagation()\n",
    "            self.test(testLabels, testData)\n",
    "            accuracy = np.sum(self.confusionMatrix.diagonal()) / \\\n",
    "                       np.sum(self.confusionMatrix)\n",
    "            self.accHistory.append(accuracy)\n",
    "            print('Accuracy = {0:.2f}%'.format(accuracy*100))\n",
    "        plt.plot(np.arange(epochs), self.accHistory)\n",
    "        plt.title(\"Training Output History\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.xlabel(\"Accuracy\")\n",
    "        plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "\n",
    "    def test(self, labels, testData):\n",
    "        self.confusionMatrix = np.zeros((\n",
    "            self.network[-1].count, self.network[-1].count))\n",
    "        for label, data in zip(labels, testData):\n",
    "            data = data.reshape(len(data), 1)\n",
    "            self.feedForward(data)\n",
    "            bestIndex = np.argmax(self.network[-1].neurons)\n",
    "            self.confusionMatrix[label, bestIndex] += 1\n",
    "    \n",
    "    def miniBatch(self, data):\n",
    "        i = 0\n",
    "        while (i < len(data)):\n",
    "            i += self.batchSize\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.feedForward(data)\n",
    "        return np.argmax(self.network[-1].neurons)\n",
    "            \n",
    "    def oneHotEncode(self, index):\n",
    "        #index = np.array(index, dtype=np.int)  # minibatch stuff\n",
    "        if(type(index) == int):\n",
    "            vect = np.zeros((self.network[-1].count, 1))\n",
    "            vect[index][0] = 1\n",
    "        else:\n",
    "            vect = np.zeros((len(index),self.network[-1].count, 1))\n",
    "            vect[range(len(index)), index, 0] = 1\n",
    "        return vect\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def standardize(self, A):\n",
    "        return (A - np.mean(A)) / np.std(A)\n",
    "\n",
    "    def normalize(self, A):\n",
    "        return (A - np.min(A)) / (np.max(A) - np.min(A))\n",
    "\n",
    "    def extractMNIST(self, fileName):\n",
    "        labels = []\n",
    "        fname = open(fileName, \"r\")\n",
    "        values = fname.readlines()[:20000]\n",
    "        fname.close()\n",
    "        for i, record in enumerate(values):\n",
    "            data = record.split(\",\")\n",
    "            values[i] = np.asfarray(data[1:]) / 255\n",
    "            labels.append(int(data[0]))\n",
    "        return labels, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Training Data\n",
      "Opening Testing Data\n",
      "Creating Network\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d3dfb430df3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Create neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating Network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-9d2d50b3a6b2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, networkStruct)\u001b[0m\n\u001b[1;32m      7\u001b[0m         np.append(np.array([self.network]),\n\u001b[1;32m      8\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mNeuronLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetworkStruct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 axis=0)\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             np.append(self.network,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5164\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5165\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "# Open files with DataLoader class methods specific for MNIST data.\n",
    "dl = DataLoader()\n",
    "print(\"Opening Training Data\")\n",
    "MNIST_Train_Labels, MNIST_Train_Values = dl.extractMNIST(\"MNIST/mnist_train.csv\")\n",
    "print(\"Opening Testing Data\")\n",
    "MNIST_Test_Labels, MNIST_Test_Values = dl.extractMNIST(\"MNIST/mnist_test.csv\")\n",
    "\n",
    "# Input parameters\n",
    "epochs = 10\n",
    "suppressOutput = False\n",
    "\n",
    "# Design neural network\n",
    "network = {0:  \n",
    "           {'neurons':     784,\n",
    "            'type':        'input'},\n",
    "           1:  \n",
    "           {'neurons':      150,\n",
    "            'learningRate': 0.001,\n",
    "            'activation':   'tanh',\n",
    "            'dropoutRate':  0.05,\n",
    "            'type':         'hidden'},\n",
    "           2:  \n",
    "#            {'neurons':      150,\n",
    "#             'learningRate': 0.001,\n",
    "#             'activation':   'tanh',\n",
    "#             'dropoutRate':  0.05,\n",
    "#             'type':         'hidden'},\n",
    "#            3:  \n",
    "           {'neurons':      10,\n",
    "            'learningRate': 0.001,\n",
    "            'activation':   'softmax',\n",
    "            'dropoutRate':  0.0,\n",
    "            'loss':         'msePrime',\n",
    "            'type':         'output'}}\n",
    "\n",
    "# Create neural network\n",
    "print(\"Creating Network\")\n",
    "ann = NeuralNetwork(network)\n",
    "\n",
    "# Train\n",
    "print(\"Training:\")\n",
    "ann.train(MNIST_Train_Labels, MNIST_Train_Values, epochs,\n",
    "          MNIST_Test_Labels, MNIST_Test_Values)\n",
    "\n",
    "# Predict \n",
    "pred = ann.predict(MNIST_Test_Values[0])\n",
    "print(\"Input : {}\".format(MNIST_Test_Labels[0]))\n",
    "print(\"Output: {}\".format(pred))\n",
    "\n",
    "# Display confusion matrix\n",
    "df = pd.DataFrame(ann.confusionMatrix / np.sum(ann.confusionMatrix) * 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "sn.heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
