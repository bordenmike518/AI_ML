{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "    def __init__(self, prevCount, struct):\n",
    "        self.prevCount = prevCount\n",
    "        self.count = struct['neurons']\n",
    "        self.neurons = None\n",
    "        self.errors = np.zeros((self.count, 1))\n",
    "        self.activationDerivative = lambda A: A\n",
    "        self.alpha = 0.25\n",
    "        if(struct['type'] != 'input'):\n",
    "            self.synapses = np.random.rand(self.count, self.prevCount)*2-1\n",
    "            self.bias = np.ones((self.count, 1))\n",
    "            self.learningRate = struct['learningRate']\n",
    "            self.dropoutRate = struct['dropoutRate']\n",
    "            self.dropout = np.array([[self.learningRate]*prevCount]*self.count)\n",
    "            if (struct['activation'] == 'identity'):\n",
    "                self.activation = lambda A: self.identity(A)\n",
    "                self.activationDerivative = lambda A: self.identity_derivative(A)\n",
    "            elif (struct['activation'] == 'binarystep'):\n",
    "                self.activation = lambda A: self.binarystep(A)\n",
    "                self.activationDerivative = lambda A: self.binarystep_derivative(A)\n",
    "            elif (struct['activation'] == 'sigmoid'):\n",
    "                self.activation = lambda A: self.sigmoid(A)\n",
    "                self.activationDerivative = lambda A: self.sigmoid_derivative(A)\n",
    "            elif (struct['activation'] == 'tanh'):\n",
    "                self.activation = lambda A: self.tanh(A)\n",
    "                self.activationDerivative = lambda A: self.tanh_derivative(A)\n",
    "            elif (struct['activation'] == 'arctan'):\n",
    "                self.activation = lambda A: self.arctan(A)\n",
    "                self.activationDerivative = lambda A: self.arctan_derivative(A)\n",
    "            elif (struct['activation'] == 'ReLU'):\n",
    "                self.activation = lambda A: self.ReLU(A)\n",
    "                self.activationDerivative = lambda A: self.ReLU_derivative(A)\n",
    "            elif (struct['activation'] == 'leakyReLU'):\n",
    "                self.activation = lambda A: self.leakyReLU(A)\n",
    "                self.activationDerivative = lambda A: self.leakyReLU_derivative(A)\n",
    "            elif (struct['activation'] == 'ELU'):\n",
    "                self.activation = lambda A: self.ELU(A)\n",
    "                self.activationDerivative = lambda A: self.ELU_derivative(A)\n",
    "            elif (struct['activation'] == 'softplus'):\n",
    "                self.activation = lambda A: self.softplus(A)\n",
    "                self.activationDerivative = lambda A: self.softplus_derivative(A)\n",
    "            elif (struct['activation'] == 'softmax'):\n",
    "                self.activation = lambda A: self.softmax(A)\n",
    "                self.activationDerivative = lambda A: self.softmax_derivative(A)\n",
    "            else:\n",
    "                print('ERROR: NeuronLayer requires valid activation.')\n",
    "\n",
    "    def identity(self, A):\n",
    "        return A\n",
    "    \n",
    "    def identity_derivative(self, A):\n",
    "        return np.ones(A.shape)\n",
    "    \n",
    "    def binarystep(self, A):\n",
    "        return np.where(A < 0, 0, 1)\n",
    "    \n",
    "    def binarystep_derivative(self, A):\n",
    "        return np.ones(A.shape)\n",
    "\n",
    "    def sigmoid(self, A):\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "\n",
    "    def sigmoid_derivative(self, A):\n",
    "        return A * (1 - A)\n",
    "\n",
    "    def tanh(self, A):\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def tanh_derivative(self, A):\n",
    "        return 1 - np.power(np.tanh(A), 2)\n",
    "\n",
    "    def arctan(self, A):\n",
    "        return np.arctan(A)\n",
    "    \n",
    "    def arctan_derivative(self, A):\n",
    "        return 1 / (np.power(A, 2) + 1)\n",
    "\n",
    "    def ReLU(self, A):\n",
    "        return np.where(A >= 0, A, 0)\n",
    "\n",
    "    def ReLU_derivative(self, A):\n",
    "        return np.where(A >= 0, 1, 0)\n",
    "\n",
    "    def leakyReLU(self, A):\n",
    "        return np.where(A >= 0, A, np.multiply(self.alpha,A))\n",
    "\n",
    "    def leakyReLU_derivative(self, A):\n",
    "        return np.where(A >= 0, 1, self.alpha)\n",
    "\n",
    "    def ELU(self, A):\n",
    "        return np.where(A >= 0, A, np.multiply(self.alpha, np.exp(A)-1))\n",
    "\n",
    "    def ELU_derivative(self, A):\n",
    "        return np.where(A >= 0, 1, np.multiply(\n",
    "            self.alpha, np.exp(A)-1) + self.alpha)\n",
    "\n",
    "    def softplus(self, A):\n",
    "        return np.log(1 + np.exp(A))\n",
    "\n",
    "    def softplus_derivative(self, A):\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "    \n",
    "    def softmax(self, A):\n",
    "        e = np.exp(A - np.max(A))\n",
    "        return e / e.sum()\n",
    "    \n",
    "    def softmax_derivative(self, A):\n",
    "        return A\n",
    "\n",
    "    def dropoutUpdate(self, level=0):\n",
    "        if (level == 0):\n",
    "            if (self.dropoutRate != 0):\n",
    "                self.dropout = self.learningRate * np.random.choice([0, 1],\n",
    "                    size=(self.count, self.prevCount),\n",
    "                    p=[self.dropoutRate, 1-self.dropoutRate]).astype(np.bool)\n",
    "        elif(level == 1):\n",
    "            np.random.shuffle(self.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, networkStruct):\n",
    "        self.network = list()\n",
    "        self.accHistory = list()\n",
    "        self.layers = len(networkStruct)\n",
    "        self.network.append(NeuronLayer(0, networkStruct[0]))\n",
    "        self.confusionMatrix = None\n",
    "        for i in range(1, self.layers):\n",
    "            self.network.append(NeuronLayer(networkStruct[i-1]['neurons'], \n",
    "                                            networkStruct[i]))\n",
    "\n",
    "    def feedForward(self, data, target=None, batchSize=1):\n",
    "        self.network[0].neurons = data\n",
    "        for i in range(1, self.layers):\n",
    "            wx = np.dot(self.network[i].synapses,\n",
    "                    self.network[i-1].neurons).T\n",
    "            wx = wx.reshape(batchSize, self.network[i].count, 1)\n",
    "            self.network[i].neurons = self.network[i].activation(\n",
    "                 wx + self.network[i].bias)\n",
    "        if (type(target) != type(None)):\n",
    "            self.network[-1].errors += np.average(\n",
    "                (target-self.network[-1].neurons) * \\\n",
    "                    self.network[-1].activationDerivative(\n",
    "                        self.network[-1].neurons), axis=0)\n",
    "\n",
    "    def backpropagation(self):\n",
    "        for i in reversed(range(1, self.layers)):\n",
    "            self.network[i-1].neurons = np.average(\n",
    "                self.network[i-1].neurons, axis=0)\n",
    "            self.network[i].synapses += self.network[i].dropout * \\\n",
    "                np.dot(self.network[i].errors,\n",
    "                    self.network[i-1].neurons.T)\n",
    "            self.network[i].bias -= np.multiply(\n",
    "                self.network[i].learningRate,\n",
    "                self.network[i].errors)\n",
    "            self.network[i-1].errors = np.multiply(np.dot(\n",
    "                self.network[i].synapses.T,\n",
    "                self.network[i].errors\n",
    "            ),  self.network[i-1].activationDerivative(self.network[i-1].neurons))\n",
    "        self.network[-1].errors = 0\n",
    "\n",
    "    def train(self, trainLabels, trainData, epochs=1, \n",
    "              testLabels=[], testData=[], batchSize=1):\n",
    "        zippedData = list(zip(trainLabels,trainData))\n",
    "        for i in range(epochs):\n",
    "            print('\\t-- Epoch {}'.format(i+1))\n",
    "            for j in range(1, self.layers):\n",
    "                self.network[j].dropoutUpdate(0)\n",
    "            np.random.shuffle(zippedData)\n",
    "            k = 0\n",
    "            while (k < len(zippedData)):\n",
    "                label, data = list(zip(*zippedData[k:k+batchSize]))\n",
    "                label, data = np.array(label), np.array([np.array(data).T]).T\n",
    "                target = self.oneHotEncode(label)\n",
    "                self.feedForward(data, target, batchSize)\n",
    "                self.backpropagation()\n",
    "                k += batchSize\n",
    "            self.test(testLabels, testData)\n",
    "            accuracy = np.sum(self.confusionMatrix.diagonal()) / \\\n",
    "                       np.sum(self.confusionMatrix)\n",
    "            self.accHistory.append(accuracy)\n",
    "            print('Accuracy = {0:.2f}%'.format(accuracy*100))\n",
    "        plt.plot(np.arange(epochs), self.accHistory)\n",
    "        plt.title(\"Training Output History\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.xlabel(\"Accuracy\")\n",
    "        plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "\n",
    "    def test(self, labels, testData):\n",
    "        self.confusionMatrix = np.zeros((\n",
    "            self.network[-1].count, self.network[-1].count))\n",
    "        for i, (label, data) in enumerate(zip(labels, testData)):\n",
    "            self.feedForward(data)\n",
    "            bestIndex = np.argmax(self.network[-1].neurons)\n",
    "            self.confusionMatrix[label, bestIndex] += 1\n",
    "    \n",
    "    def miniBatch(self, data, batchSize=100):\n",
    "        i = 0\n",
    "        while (i < len(data)):\n",
    "            i += batchSize\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.feedForward(data)\n",
    "        return np.argmax(self.network[-1].neurons)\n",
    "            \n",
    "    def oneHotEncode(self, index):\n",
    "        index = np.array(index, dtype=np.int)\n",
    "        if(type(index) == int):\n",
    "            vect = np.zeros((self.network[-1].count, 1))\n",
    "            vect[index][0] = 1\n",
    "        else:\n",
    "            vect = np.zeros((len(index),self.network[-1].count, 1))\n",
    "            vect[range(len(index)), index, 0] = 1\n",
    "        return vect\n",
    "    \n",
    "    def logLikelihood(self, y, yhat):\n",
    "        return np.where((y/2)-0.5+yhat > 0, -np.log(np.abs((y/2)-0.5+yhat)), 0)\n",
    "    \n",
    "    def entropy(p):\n",
    "        return -np.sum(p*np.log2(p))\n",
    "        \n",
    "    def crossEntropy(self, p, q):\n",
    "        '''\n",
    "        p = true probability distribution (expected)\n",
    "        q = predicted probability distribution (guessed)\n",
    "        '''\n",
    "        return -np.sum(np.where(q > 0, p*np.log2(q), 0))\n",
    "\n",
    "    def KLDivergence(self, p, q):\n",
    "        return self.crossEntropy(p,q) - self.entropy(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def standardize(self, A):\n",
    "        return (A - np.mean(A)) / np.std(A)\n",
    "\n",
    "    def normalize(self, A):\n",
    "        return (A - np.min(A)) / (np.max(A) - np.min(A))\n",
    "\n",
    "    def extractMNIST(self, fileName):\n",
    "        labels = []\n",
    "        fname = open(fileName, \"r\")\n",
    "        values = fname.readlines()[:20000]\n",
    "        fname.close()\n",
    "        for i, record in enumerate(values):\n",
    "            data = record.split(\",\")\n",
    "            values[i] = np.asfarray(data[1:]) / 255\n",
    "            labels.append(int(data[0]))\n",
    "        return labels, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network\n",
      "Opening Training Data\n",
      "Opening Testing Data\n",
      "Training:\n",
      "\t-- Epoch 1\n",
      "Accuracy = 30.83%\n",
      "\t-- Epoch 2\n",
      "Accuracy = 43.58%\n",
      "\t-- Epoch 3\n",
      "Accuracy = 56.28%\n",
      "\t-- Epoch 4\n",
      "Accuracy = 63.23%\n",
      "\t-- Epoch 5\n",
      "Accuracy = 69.02%\n",
      "\t-- Epoch 6\n",
      "Accuracy = 74.33%\n",
      "\t-- Epoch 7\n",
      "Accuracy = 77.63%\n",
      "\t-- Epoch 8\n",
      "Accuracy = 79.12%\n",
      "\t-- Epoch 9\n",
      "Accuracy = 80.54%\n",
      "\t-- Epoch 10\n",
      "Accuracy = 81.26%\n",
      "\t-- Epoch 11\n",
      "Accuracy = 82.35%\n",
      "\t-- Epoch 12\n",
      "Accuracy = 83.07%\n",
      "\t-- Epoch 13\n",
      "Accuracy = 83.51%\n",
      "\t-- Epoch 14\n",
      "Accuracy = 84.01%\n",
      "\t-- Epoch 15\n",
      "Accuracy = 84.17%\n",
      "\t-- Epoch 16\n",
      "Accuracy = 84.79%\n",
      "\t-- Epoch 17\n",
      "Accuracy = 84.83%\n",
      "\t-- Epoch 18\n",
      "Accuracy = 84.93%\n",
      "\t-- Epoch 19\n",
      "Accuracy = 85.25%\n",
      "\t-- Epoch 20\n",
      "Accuracy = 85.45%\n",
      "\t-- Epoch 21\n",
      "Accuracy = 85.85%\n",
      "\t-- Epoch 22\n",
      "Accuracy = 85.80%\n",
      "\t-- Epoch 23\n",
      "Accuracy = 85.87%\n",
      "\t-- Epoch 24\n",
      "Accuracy = 85.64%\n",
      "\t-- Epoch 25\n",
      "Accuracy = 86.14%\n",
      "\t-- Epoch 26\n",
      "Accuracy = 86.24%\n",
      "\t-- Epoch 27\n",
      "Accuracy = 85.86%\n",
      "\t-- Epoch 28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "# Design neural network\n",
    "network = {0:  \n",
    "           {'neurons':     784,\n",
    "            'type':       'input'},\n",
    "           1:  \n",
    "           {'neurons':     250,\n",
    "            'learningRate': 0.001,\n",
    "            'activation': 'tanh',\n",
    "            'dropoutRate': 0.01,\n",
    "            'type':       'hidden'},\n",
    "           2:  \n",
    "#            {'neurons':     150,\n",
    "#             'learningRate': 0.001,\n",
    "#             'activation': 'tanh',\n",
    "#             'dropoutRate': 0.01,\n",
    "#             'type':       'hidden'},\n",
    "#            3:  \n",
    "           {'neurons':     10,\n",
    "            'learningRate': 0.001,\n",
    "            'activation': 'softmax',\n",
    "            'dropoutRate': 0.01,\n",
    "            'type':       'output'}}\n",
    "\n",
    "# Input parameters\n",
    "epochs = 30\n",
    "suppressOutput = False\n",
    "\n",
    "# Create neural network\n",
    "print(\"Creating Network\")\n",
    "ann = NeuralNetwork(network)\n",
    "\n",
    "# Open files with DataLoader class methods specific for MNIST data.\n",
    "dl = DataLoader()\n",
    "print(\"Opening Training Data\")\n",
    "MNIST_Train_Labels, MNIST_Train_Values = dl.extractMNIST(\"MNIST/mnist_train.csv\")\n",
    "print(\"Opening Testing Data\")\n",
    "MNIST_Test_Labels, MNIST_Test_Values = dl.extractMNIST(\"MNIST/mnist_test.csv\")\n",
    "\n",
    "# Train\n",
    "print(\"Training:\")\n",
    "ann.train(MNIST_Train_Labels, MNIST_Train_Values, epochs,\n",
    "          MNIST_Test_Labels, MNIST_Test_Values, 1)\n",
    "\n",
    "# Predict \n",
    "pred = ann.predict(MNIST_Test_Values[0])\n",
    "print(\"Input : {}\".format(MNIST_Test_Labels[0]))\n",
    "print(\"Output: {}\".format(pred))\n",
    "df = pd.DataFrame(ann.confusionMatrix / np.sum(ann.confusionMatrix) * 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "sn.heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
